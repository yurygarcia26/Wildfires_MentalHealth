{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<center>\n","\n","# **DATA PROCESSING**\n","\n","</center>\n","\n","With this spcrip we:\n","\n","**Step 1:**\n","Concatenate all the files created when we gather the data.\n","Create a new column in each folder called \"clean_text\" and save in a new folder (Cleaned_data) the files that do not have any error.\n","\n","The cleaning process:\n","\n","-\tRemove twits with NaN in the text.\n","-\tRemove # and separtate hastags (CaliforniaWildfire = California Wildfire)\n","-\tRemove URL\n","-\tRemove special characters and punctuation.\n","- Lowercassing\n","-\tRemove stop words, however we exclude negation stop words because those words provide us information about negative sentiments.\n","-\tTokenization\n","\n","**Step 2:**\n","We join all the files in only one.\n","\n","**Step 3:**\n","A little more cleaning\n","-\tRemove duplicated twits.\n","-\tRemove tweets with a date in the text.\n","- Filter twitts in english only\n","\n","At the end, this script create a concatenated file with all the cleaned results. The name of the file is \"WildfireName_data.csv\" and it is saved in the folder DataProcessing"],"metadata":{"id":"V-gG0dbgFlEe"}},{"cell_type":"code","source":["import os\n","import glob\n","import pandas as pd\n","import re\n","import csv\n","import re\n","import string\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n"],"metadata":{"id":"jkBFAjVeVEpv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BJ7L2W4VAjy"},"outputs":[],"source":["#Mount your Google Drive to Colab\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Chose the wildifire to analyze"],"metadata":{"id":"bkD7WRn8E2N_"}},{"cell_type":"code","source":["#name for the final file\n","Wildfire_name = '1.TubbsFire'\n","folder        = 'ResultsTubbsFire/'\n","final_folder  = 'ResultsTubbsFire/Cleaned_files/'\n"],"metadata":{"id":"WxD5LFkeEPjJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set folder and output paths"],"metadata":{"id":"9--8REEyE8zK"}},{"cell_type":"code","source":["folder_path    = '/content/drive/MyDrive/Mental_Health_Wildfire/Twitter_Data/Tubbs_Codes/1.CollectingData/'+folder  # change this to the path of the folder containing the CSV files\n","output_path    = '/content/drive/MyDrive/Mental_Health_Wildfire/Twitter_Data/Tubbs_Codes/1.CollectingData/'+final_folder  # change this to the desired output file path\n","\n","os.listdir('/content/drive/MyDrive/Mental_Health_Wildfire/Twitter_Data/Tubbs_Codes/1.CollectingData/')"],"metadata":{"id":"wBVbtpeXEP3_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the data columns types\n","dtypes = {\n","    \"tweet_id\": \"object\",\n","    \"tweet_text\":\"object\",\n","    \"tweet_possibly_sensitive\": \"object\",\n","    \"tweet_text\": \"object\",\n","    \"tweet_source\": \"object\",\n","    \"tweet_lang\": \"object\",\n","    \"tweet_retweet_count\": \"object\",\n","    \"tweet_reply_count\":\"object\",\n","    \"tweet_like_count\": \"object\",\n","    \"tweet_quote_count\": \"object\",\n","    \"tweet_impression_count\": \"object\",\n","    \"user_id\":\"object\",\n","    \"user_username\": \"object\",\n","    \"user_verified\":\"object\",\n","    \"user_protected\":\"object\",\n","    \"user_description\":\"object\",\n","    \"user_profile_image_url\":\"object\",\n","    \"user_location\":\"object\",\n","    \"user_followers_count\":\"object\",\n","    \"user_friends_count\":\"object\",\n","    \"user_tweet_count\":\"object\",\n","    \"place_id\":\"object\",\n","    \"place_name\": \"object\",\n","    \"place_full_name\":\"object\",\n","    \"place_country\":\"object\",\n","    \"place_country_code\":\"object\",\n","    \"place_type\":\"object\"\n","}"],"metadata":{"id":"VuL70ejNujxP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 1:** First clean phase\n","\n","-\tRemove twits with NaN in the text.\n","- Remove # and separtate hastags (CaliforniaWildfire = California Wildfire)\n","-\tRemove URL\n","-\tRemove special characters and punctuation.\n","-\tRemove stop words, however we exclude negation stop words because those words provide us information about negative sentiments.\n","-\tTokenization\n","\n"],"metadata":{"id":"rJvIhzEyW8-S"}},{"cell_type":"code","source":["def preprocess_text(text):\n","\n","    #separate words in the hashtagas and remove #\n","    hashtags = re.findall(r'#\\w+', text)\n","\n","    # Iterate over hashtags and separate words\n","    for hashtag in hashtags:\n","        separated_words = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', hashtag[1:])\n","\n","        # Replace the hashtag with separated words\n","        text = text.replace(hashtag, separated_words)\n","\n","    # Remove URLs\n","    text = re.sub(r'http\\S+', '', text)\n","    # Remove mentions and hashtags\n","    text = re.sub(r'@\\w+|#\\w+', '', text)\n","    # Remove special characters and punctuation\n","    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n","\n","    # Tokenization\n","    tokens = word_tokenize(text)\n","    # Lowercasing\n","    tokens = [token.lower() for token in tokens]\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","\n","    # Define stopwords related to negation\n","    negation_stopwords = {'not', 'no', 'don', \"don't\", 'won', \"won't\", 'cannot', 'couldn', \"couldn't\",'aren',\n","     \"aren't\",'didn', \"didn't\", 'doesn', \"doesn't\",'hadn', \"hadn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'needn',  \"needn't\",  'nor', 'weren',  \"weren't\", 'weren',  \"weren't\",'wouldn',\n","    \"wouldn't\", 'hasn', \"hasn't\",'mustn', \"mustn't\",'shan', \"shan't\", 'shouldn',  \"shouldn't\", 'wasn',  \"wasn't\"}\n","\n","    # Exclude negation stopwords from the set\n","    filtered_stop_words = stop_words - negation_stopwords\n","    tokens = [token for token in tokens if not token in stop_words]\n","    return ' '.join(tokens)"],"metadata":{"id":"P6U6Kn_pWCHJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#The files are saved in a new folder called Cleaned_files\n","\n","list_bad_file_name = []\n","\n","for file_name in os.listdir(folder_path):\n","    if file_name.endswith(\".csv\"):\n","        try:\n","            file_path = os.path.join(folder_path, file_name)\n","            file_size = os.stat(file_path).st_size\n","            if file_size == 0:\n","                print(\"Error: Empty file -\", file_name, \"- Skipping the file.\")\n","                list_bad_file_name.append(file_name)\n","                continue\n","\n","            with open(file_path) as f:\n","                first_line = f.readline().strip()  # Read the first line\n","            if not first_line:  # Empty first line\n","                print(\"Error: No columns to parse from file -\", file_name, \"- Skipping the file.\")\n","                list_bad_file_name.append(file_name)\n","                continue\n","\n","            df = pd.read_csv(file_path)\n","\n","            #remove twitts with NaN in the text\n","            df= df.dropna(subset=['tweet_text'])\n","\n","            #Remove RT\n","            df = df[~df['tweet_text'].str.startswith('RT')]\n","\n","\n","            # convert columns to desired data types\n","            df[\"tweet_id\"]                   = df[\"tweet_id\"].astype(object)\n","           # df[\"tweet_created_at\"]           = pd.to_datetime(df[\"tweet_created_at\"])\n","            df[\"tweet_created_at\"]           = df[\"tweet_created_at\"].astype(object)\n","            df[\"tweet_possibly_sensitive\"]   = df[\"tweet_possibly_sensitive\"].astype(object)\n","            df[\"tweet_text\"]                 = df[\"tweet_text\"].astype(object)\n","            df[\"tweet_source\"]               = df[\"tweet_source\"].astype(object)\n","            df[\"tweet_lang\"]                 = df[\"tweet_lang\"].astype(object)\n","            df[\"tweet_retweet_count\"]        = df[\"tweet_retweet_count\"].astype(object)\n","            df[\"tweet_reply_count\"]          = df[\"tweet_reply_count\"].astype(object)\n","            df[\"tweet_like_count\"]           = df[\"tweet_like_count\"].astype(object)\n","            df[\"tweet_quote_count\"]          = df[\"tweet_quote_count\"].astype(object)\n","            df[\"tweet_impression_count\"]     = df[\"tweet_impression_count\"].astype(object)\n","            df[\"user_id\"]                    = df[\"user_id\"].astype(object)\n","            df[\"user_username\"]              = df[\"user_username\"].astype(object)\n","            df[\"user_verified\"]              = df[\"user_verified\"].astype(object)\n","            df[\"user_protected\"]             = df[\"user_protected\"].astype(object)\n","            df[\"user_description\"]           = df[\"user_description\"].astype(object)\n","            df[\"user_profile_image_url\"]     = df[\"user_profile_image_url\"].astype(object)\n","            df[\"user_location\"]              = df[\"user_location\"].astype(object)\n","            df[\"user_followers_count\"]       = df[\"user_followers_count\"].astype(object)\n","            df[\"user_friends_count\"]         = df[\"user_friends_count\"].astype(object)\n","            df[\"user_tweet_count\"]           = df[\"user_tweet_count\"].astype(object)\n","            df[\"place_id\"]                   = df[\"place_id\"].astype(object)\n","            df[\"place_name\"]                 = df[\"place_name\"].astype(object)\n","            df[\"place_full_name\"]            = df[\"place_full_name\"].astype(object)\n","            df[\"place_country\"]              = df[\"place_country\"].astype(object)\n","            df[\"place_country_code\"]         = df[\"place_country_code\"].astype(object)\n","            df[\"place_type\"]                 = df[\"place_type\"].astype(object)\n","\n","            #df[\"tweet_text\"] = df[\"tweet_text\"].apply(lambda x: re.sub(pattern, \"\", x))\n","\n","            df['clean_text']  = df['tweet_text'].apply(preprocess_text)\n","            output_file_name  = \"cleaned_\" + file_name\n","            df.to_csv(os.path.join(output_path, output_file_name), index=False)\n","        except pd.errors.ParserError:\n","            list_bad_file_name.append(file_name)\n","            print(\"Error in file:\", file_name, \"- Skipping the file.\")"],"metadata":{"id":"CFvZjFUsSC8i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 2:**  Create only one file.\n","\n","We take all the files saved in cleaned_files to create only one.\n","The new file is saved in the *DataProcessing* folder"],"metadata":{"id":"T5t6e37Pr9fS"}},{"cell_type":"code","source":["# Create an empty list to store DataFrames\n","#output_path    = '/content/drive/MyDrive/Mental_Health_Wildfire/Twitter_Data/Tubbs_Codes/1.Collecting_data/'+final_folder  # change this to the desired output file path\n","dfs = []\n","\n","# Loop through each CSV file in the folder\n","for file_name in os.listdir(output_path):\n","    if file_name.endswith(\".csv\"):\n","        # Load the CSV file into a Pandas DataFrame\n","        df = pd.read_csv(os.path.join(output_path, file_name),dtype=dtypes)\n","        #print(df.shape)\n","\n","        # Append the DataFrame to the list\n","        dfs.append(df)\n","\n","# Concatenate all DataFrames in the list into a single DataFrame\n","df_concatenated = pd.concat(dfs, ignore_index=True)\n","\n","print(df_concatenated.shape)\n","df_original = df_concatenated\n","\n","df_original.shape"],"metadata":{"id":"Dylk8heuXG1S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 3:** A litle more clining"],"metadata":{"id":"s4aR4BX_JBxJ"}},{"cell_type":"code","source":["#remove twitts with NaN in the text\n","df_concatenated= df_concatenated.dropna(subset=['clean_text'])\n","\n","#remove dupplicated twitts\n","df_concatenated = df_concatenated.drop_duplicates(subset=['tweet_id'], keep='first')\n","\n","#remove tweets with a date in the text\n","# Define a regular expression pattern to match the format of a date\n","date_pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}\\+\\d{2}:\\d{2}\")\n","\n","# Iterate through each tweet in the DataFrame\n","for index, row in df_concatenated.iterrows():\n","    tweet_text = row[\"tweet_text\"]\n","\n","    # Check if the tweet text matches the date pattern\n","    if date_pattern.match(tweet_text):\n","        # Remove the tweet from the DataFrame\n","        df_concatenated.drop(index, inplace=True)\n","\n","print(df_concatenated.shape)"],"metadata":{"id":"Cj6TFBz1SG4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter out tweets starting with 'RT'\n","df_concatenated = df_concatenated[~df_concatenated['tweet_text'].str.startswith('RT')]\n","df_concatenated.head(5)\n","print(df_concatenated.shape)"],"metadata":{"id":"sQUbLBerfCJe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# * Check languages\n","\n","We are going to keep only tweets in english"],"metadata":{"id":"wbS8sXC0gUXD"}},{"cell_type":"code","source":["filtered_df2 = df_concatenated\n","#check the languages\n","column_data = filtered_df2['tweet_lang']\n","\n","# Filter out missing values and convert non-string values to strings\n","column_data = column_data.dropna().astype(str)\n","\n","# Join the column values and split into individual words\n","all_words = ' '.join(column_data).split()\n","\n","# Create a set of unique words\n","unique_words = set(all_words)\n","\n","# Display the unique words\n","print(unique_words)"],"metadata":{"id":"gU8PUKGWgSpN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#use this line to see the texts of specific language observed in the previous line of code\n","\n","filtered_df2[(filtered_df2['tweet_lang'].str.contains('en', na=False))]"],"metadata":{"id":"MKgbjtshgSyZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find the index position by index label and remove data in 'it'\n","index_position = filtered_df2.loc[filtered_df2['tweet_lang'].str.contains('en',na=False)].index\n","df_english     = filtered_df2.loc[index_position]\n","\n","print(len(index_position))\n","print(df_english.shape)"],"metadata":{"id":"RDgUo-TzgS7X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# * Tokenization\n","\n","Save the tokens in a new column called \"tokens\""],"metadata":{"id":"zZVgY9kTgdiM"}},{"cell_type":"code","source":["from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","import nltk"],"metadata":{"id":"PMQH5e5NgTDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize and preprocess the text\n","def tokenize(text):\n","    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n","\n","#Remove na from clean text\n","df_english           = df_english.dropna(subset=['clean_text'])\n","df_english['tokens'] = df_english['clean_text'].apply(tokenize)"],"metadata":{"id":"Un1Qb4fUgTJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_english['tokens']\n","df_english.shape"],"metadata":{"id":"9rJlCGEvH3CR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the file in Data folder\n","join_files_path= '/content/drive/MyDrive/Mental_Health_Wildfire/Twitter_Data/Tubbs_Codes/Data/'\n","\n","output_file_name = Wildfire_name+\"_version1.csv\"\n","df_english.to_csv(os.path.join(join_files_path,output_file_name), index=False)"],"metadata":{"id":"BvYRtRssSHvh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["join_files_path"],"metadata":{"id":"fcB2gbpUHLEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_english['tweet_created_at'].max()\n","\n","df_english.shape"],"metadata":{"id":"Mr8DjwuWXlhA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Save file:**\n"],"metadata":{"id":"XrEr_e4vhWSk"}},{"cell_type":"code","source":["# Save the concatenated DataFrame to a new CSV file\n","\n","output_file_name2 = \"1.Tubbs_version1.csv\"\n","df_english.to_csv(os.path.join(join_files_path,output_file_name2), index=False)"],"metadata":{"id":"YZvVol6EM0Jl"},"execution_count":null,"outputs":[]}]}